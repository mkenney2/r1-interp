# Phase 2: Full transcoder training for all 28 layers
# Hyperparams are placeholders â€” update with sweep winners from Phase 1.
# Run: python scripts/train.py --config configs/train_full.yaml

type: TrainRunConfig

hyperparams:
  expansion_factor: 64       # UPDATE after sweep
  top_k: 64                  # UPDATE after sweep
  skip_connection: true
  learning_rate: 3.0e-4
  training_tokens: 200000000
  batch_size: 32               # sequences, not tokens
  warmup_steps: 1000
  weight_decay: 0.0

layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27]

dataset: open-r1/OpenR1-Math-220k
wandb_project: r1-interp-train
checkpoint_dir: checkpoints
distribute_modules: true
num_gpus: 8
