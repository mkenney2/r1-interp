# Phase 2: Full transcoder training for all 28 layers
# Sweep winners: exp=32, k=64 (CE incr=0.020, dead=34%, L0=64)
# Run: python scripts/train.py --config configs/train_full.yaml
#
# Estimated time: ~6 hours/layer × 28 layers = ~7 days on 1× A100 80GB

type: TrainRunConfig

hyperparams:
  expansion_factor: 32       # sweep winner
  top_k: 64                  # sweep winner
  skip_connection: true
  learning_rate: 3.0e-4
  training_tokens: 200000000
  batch_size: 8                # micro-batch sequences
  grad_acc_steps: 4            # effective batch = 8 * 4 = 32
  warmup_steps: 1000
  weight_decay: 0.0

layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27]

dataset: Skylion007/openwebtext
wandb_project: r1-interp-train
checkpoint_dir: checkpoints
distribute_modules: true
num_gpus: 1
