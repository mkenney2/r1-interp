# Phase 1: Hyperparameter sweep on 3 pilot layers
# Run: python scripts/sweep.py --config configs/sweep_pilot.yaml

type: SweepConfig

layers: [2, 14, 26]
expansion_factors: [32, 64, 128]
top_k_values: [32, 64, 128]
skip_connection: true
learning_rate: 3.0e-4
training_tokens: 50000000    # 50M (enough to compare configs)
batch_size: 8                # micro-batch sequences
grad_acc_steps: 4            # effective batch = 8 * 4 = 32
warmup_steps: 1000

dataset: Skylion007/openwebtext
wandb_project: r1-interp-sweep
